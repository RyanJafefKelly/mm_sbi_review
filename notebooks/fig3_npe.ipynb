{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import random as random\n",
    "from functools import partial\n",
    "\n",
    "from mm_sbi_review.examples.misspec_ma1 import assumed_dgp, calculate_summary_statistics, true_dgp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from sbi.inference import SNPE\n",
    "from sbi.utils import BoxUniform\n",
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "# Ensure reproducibility\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "# Define the prior over θ in [-1, 1]\n",
    "prior = BoxUniform(low=torch.tensor([-1.0]), high=torch.tensor([1.0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the assumed data-generating process (MA(1) model)\n",
    "def assumed_dgp(theta, n_obs=100):\n",
    "    \"\"\"\n",
    "    Simulate an MA(1) process.\n",
    "\n",
    "    Args:\n",
    "        theta (torch.Tensor): Shape [batch_size, 1]\n",
    "        n_obs (int): Number of observations.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Simulated data, shape [batch_size, n_obs]\n",
    "    \"\"\"\n",
    "    batch_size = theta.shape[0]\n",
    "    w = torch.randn(batch_size, n_obs + 2)\n",
    "    x = w[:, 2:] + theta * w[:, 1:-1]\n",
    "    return x\n",
    "\n",
    "# Define the autocovariance function\n",
    "def autocov(x, lag=1):\n",
    "    \"\"\"\n",
    "    Compute the autocovariance at a given lag.\n",
    "\n",
    "    Args:\n",
    "        x (torch.Tensor): Shape [batch_size, n_obs]\n",
    "        lag (int): The lag at which to compute the autocovariance.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Shape [batch_size]\n",
    "    \"\"\"\n",
    "    if lag == 0:\n",
    "        C = torch.mean(x ** 2, dim=1)\n",
    "    else:\n",
    "        C = torch.mean(x[:, lag:] * x[:, :-lag], dim=1)\n",
    "    return C\n",
    "\n",
    "# Define the function to calculate summary statistics\n",
    "def calculate_summary_statistics(x):\n",
    "    \"\"\"\n",
    "    Calculate summary statistics for misspecified MA(1) example.\n",
    "\n",
    "    Args:\n",
    "        x (torch.Tensor): Shape [batch_size, n_obs]\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Shape [batch_size, 2]\n",
    "    \"\"\"\n",
    "    s0 = autocov(x, lag=0)\n",
    "    s1 = autocov(x, lag=1)\n",
    "    return torch.stack([s0, s1], dim=1)\n",
    "\n",
    "\n",
    "# Define the true data-generating process (stochastic volatility model)\n",
    "def true_dgp(w=-0.736, rho=0.9, sigma_v=0.36, batch_size=1, n_obs=100):\n",
    "    \"\"\"\n",
    "    Sample from a stochastic volatility model.\n",
    "\n",
    "    Args:\n",
    "        w (float): Model parameter.\n",
    "        rho (float): AR coefficient.\n",
    "        sigma_v (float): Standard deviation of the volatility process.\n",
    "        batch_size (int): Number of sequences to generate.\n",
    "        n_obs (int): Number of observations in each sequence.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Generated samples, shape [batch_size, n_obs]\n",
    "    \"\"\"\n",
    "    w_vec = torch.full((batch_size,), w)\n",
    "    rho_vec = torch.full((batch_size,), rho)\n",
    "    sigma_v_vec = torch.full((batch_size,), sigma_v)\n",
    "\n",
    "    h_mat = torch.zeros(batch_size, n_obs)\n",
    "    y_mat = torch.zeros(batch_size, n_obs)\n",
    "\n",
    "    # Initial value\n",
    "    h_mat[:, 0] = w_vec + torch.randn(batch_size) * sigma_v_vec\n",
    "    y_mat[:, 0] = torch.exp(h_mat[:, 0] / 2) * torch.randn(batch_size)\n",
    "\n",
    "    for i in range(1, n_obs):\n",
    "        h_mat[:, i] = w_vec + rho_vec * h_mat[:, i - 1] + torch.randn(batch_size) * sigma_v_vec\n",
    "        y_mat[:, i] = torch.exp(h_mat[:, i] / 2) * torch.randn(batch_size)\n",
    "\n",
    "    return y_mat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_obs = 100\n",
    "# Generate observed data\n",
    "x_o = true_dgp(n_obs=n_obs).squeeze(0)  # Shape [n_obs]\n",
    "x_o_ss = calculate_summary_statistics(x_o.unsqueeze(0))  # Shape [1, 2]\n",
    "# Number of simulations\n",
    "num_simulations = 10_000\n",
    "\n",
    "# Sample parameters θ from the prior\n",
    "theta = prior.sample((num_simulations,))  # Shape [num_simulations, 1]\n",
    "\n",
    "# Simulate data and calculate summary statistics\n",
    "x_sim = assumed_dgp(theta, n_obs=n_obs)  # Shape [num_simulations, n_obs]\n",
    "x_ss = calculate_summary_statistics(x_sim)  # Shape [num_simulations, 2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the simulator function for sbi\n",
    "# def simulator(theta, n_obs=100):\n",
    "#     \"\"\"\n",
    "#     Simulator function for sbi.\n",
    "\n",
    "#     Args:\n",
    "#         theta (torch.Tensor): Shape [batch_size, 1]\n",
    "\n",
    "#     Returns:\n",
    "#         torch.Tensor: Summary statistics, shape [batch_size, 2]\n",
    "#     \"\"\"\n",
    "#     x_sim = assumed_dgp(theta, n_obs=n_obs)\n",
    "#     x_ss = calculate_summary_statistics(x_sim)\n",
    "#     return x_ss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ryankelly/python_projects/mm_sbi_review/.venv/lib/python3.11/site-packages/sbi/neural_nets/net_builders/flow.py:141: UserWarning: In one-dimensional output space, this flow is limited to Gaussians\n",
      "  x_numel = get_numel(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Neural network successfully converged after 55 epochs."
     ]
    }
   ],
   "source": [
    "# Initialize the NPE inference method\n",
    "inference = SNPE(prior=prior)\n",
    "\n",
    "# Train the neural network\n",
    "density_estimator = inference.append_simulations(theta, x_ss).train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04466b285dc6477786c50e7b02408a29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Build the posterior distribution\n",
    "posterior = inference.build_posterior(density_estimator)\n",
    "\n",
    "# Sample from the posterior given the observed data summary statistics\n",
    "num_posterior_samples = 10_000\n",
    "samples = posterior.sample((num_posterior_samples,), x=x_o_ss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert samples to NumPy array for plotting\n",
    "# samples_np = samples.numpy().flatten()\n",
    "# print('a')\n",
    "# # Plot the posterior samples\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# plt.hist(samples_np, bins=50, density=True, alpha=0.6, label='NPE Posterior Samples')\n",
    "# print('b')\n",
    "\n",
    "# # Plot the prior distribution\n",
    "# theta_range = np.linspace(-1, 1, 200)\n",
    "# prior_density = np.ones_like(theta_range) * 0.5  # Uniform density\n",
    "# plt.plot(theta_range, prior_density, label='Prior', color='black', linestyle='--')\n",
    "# print('c')\n",
    "\n",
    "# # Plot the pseudo-true parameter value\n",
    "# # Since the model is misspecified, we may consider a pseudo-true value\n",
    "# theta_pseudo_true = 0.0  # Adjust based on your context\n",
    "# plt.axvline(theta_pseudo_true, color='red', linestyle='--', label='Pseudo-true θ')\n",
    "\n",
    "# plt.xlabel(r'$\\theta$')\n",
    "# plt.ylabel('Density')\n",
    "# plt.title('Posterior Distribution of $\\theta$ using NPE')\n",
    "# plt.legend()\n",
    "# plt.tight_layout()\n",
    "# plt.savefig(\"figs/npe_posterior.pdf\")\n",
    "# plt.clf()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot the joint distribution of (s0, s1)\n",
    "\n",
    "# x_pp_ss = np.empty((1000, 2))\n",
    "\n",
    "# for i in range(1000):\n",
    "#     theta = samples[i, :]\n",
    "#     x_sim = assumed_dgp(torch.atleast_2d(theta), n_obs=n_obs)\n",
    "#     x_ss = calculate_summary_statistics(x_sim)\n",
    "#     x_pp_ss[i, :] = x_ss\n",
    "\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# plt.scatter(x_pp_ss[:, 1], x_pp_ss[:, 0], alpha=0.5, label='Posterior Predictive')\n",
    "# plt.scatter(x_o_ss[0, 1].item(), x_o_ss[0, 0].item(), color='red', marker='x', s=100, label='Observed Data')\n",
    "# plt.xlabel('$s_0$ (Autocovariance at lag 0)')\n",
    "# plt.ylabel('$s_1$ (Autocovariance at lag 1)')\n",
    "# plt.title('Posterior Predictive Summary Statistics')\n",
    "# plt.legend()\n",
    "# plt.tight_layout()\n",
    "# plt.savefig(\"figs/posterior_predictive_joint_npe.pdf\")\n",
    "# plt.clf()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Update font size for consistency\n",
    "plt.rcParams.update({'font.size': 24})\n",
    "\n",
    "# Assume that `samples` contains the posterior samples from NPE\n",
    "# and that `assumed_dgp` and `calculate_summary_statistics` are defined\n",
    "# Also, `x_o_ss` contains the observed summary statistics\n",
    "\n",
    "# Convert samples to NumPy array for plotting\n",
    "samples_np = samples.numpy().flatten()\n",
    "\n",
    "# Plot the histogram of posterior samples\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.hist(samples_np, bins=50, density=True, alpha=0.6, color='blue', label='Posterior Samples')\n",
    "\n",
    "# Plot the prior distribution for comparison\n",
    "theta_range = np.linspace(-1, 1, 200)\n",
    "\n",
    "# Plot the pseudo-true parameter value\n",
    "theta_pseudo_true = 0.0  # Adjust based on your context\n",
    "plt.axvline(theta_pseudo_true, color='black', linestyle='--', label='Pseudo-true θ')\n",
    "\n",
    "# Add labels and legend\n",
    "plt.xlabel(r'$\\theta$')\n",
    "plt.ylabel('Density')\n",
    "plt.xlim([-1, 1])\n",
    "plt.xticks([-1, 0, 1])\n",
    "# plt.title('Posterior Distribution of $\\theta$ using NPE')\n",
    "plt.legend(fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"figs/fig3a_npe.pdf\")\n",
    "plt.clf()\n",
    "\n",
    "# Posterior predictive simulations\n",
    "num_posterior_samples = len(samples_np)\n",
    "num_pp_samples = 1_000  # Number of posterior predictive samples\n",
    "thinning_interval = num_posterior_samples // num_pp_samples\n",
    "\n",
    "# Initialize array to store summary statistics\n",
    "x_pp_ss = np.empty((num_pp_samples, 2))\n",
    "\n",
    "# Perform posterior predictive simulations\n",
    "for i in range(num_pp_samples):\n",
    "    idx = i * thinning_interval\n",
    "    theta = samples_np[idx]\n",
    "    x_sim = assumed_dgp(torch.tensor([[theta]]), n_obs=100)  # Adjust n_obs as needed\n",
    "    x_ss = calculate_summary_statistics(x_sim)\n",
    "    x_pp_ss[i, :] = x_ss.numpy()\n",
    "\n",
    "def b_theta(t):\n",
    "    return np.array([1 + t ** 2, t])\n",
    "\n",
    "# Generate values for b_theta\n",
    "t_vals = np.linspace(-1, 1, 101)\n",
    "b_theta_vals = np.array([b_theta(t) for t in t_vals])\n",
    "\n",
    "\n",
    "# Plot the joint distribution of summary statistics from posterior predictive simulations\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(x_pp_ss[:, 1], x_pp_ss[:, 0], c='blue', alpha=0.5)\n",
    "plt.plot(b_theta_vals[:, 1], b_theta_vals[:, 0], color='orange', label=r'$b(\\theta)$', linewidth=6)\n",
    "plt.xlim(-2.0, 2.0)\n",
    "plt.ylim(-0.5, 2.5)\n",
    "plt.xticks([-2, 0, 2])\n",
    "plt.yticks([-0.5, 1, 2.5])\n",
    "\n",
    "# Plot the observed summary statistics\n",
    "plt.scatter(x_o_ss[0, 1].item(), x_o_ss[0, 0].item(), color='black', marker='x', s=100, label='S(y)')\n",
    "\n",
    "# Include the function b_theta for reference, if applicable\n",
    "# plt.plot(b_theta_vals[:, 1], b_theta_vals[:, 0], color='orange', label=r'$b(\\theta)$', linewidth=4)\n",
    "\n",
    "# Set axis limits and labels\n",
    "plt.xlabel(r'$\\zeta_1$')\n",
    "plt.ylabel(r'$\\zeta_2$')\n",
    "# plt.title('Posterior Predictive Summary Statistics')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"figs/fig3b_npe.pdf\")\n",
    "plt.clf()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 1/1\n",
      " Neural network successfully converged after 57 epochs."
     ]
    }
   ],
   "source": [
    "from sbi.inference import SNLE\n",
    "\n",
    "inference = SNLE(prior=prior)\n",
    "# Number of rounds and simulations\n",
    "num_rounds = 1  # Adjust as needed\n",
    "num_sims = 10_000  # Number of simulations per round\n",
    "\n",
    "# Set initial proposal\n",
    "proposal = prior\n",
    "\n",
    "for round_ in range(num_rounds):\n",
    "    print(f\"Round {round_ + 1}/{num_rounds}\")\n",
    "    # Sample θ from the proposal\n",
    "    theta = proposal.sample((num_sims,))\n",
    "    \n",
    "    # Simulate data and calculate summary statistics\n",
    "    x_sim = assumed_dgp(theta, n_obs=n_obs)\n",
    "    x_ss = calculate_summary_statistics(x_sim)\n",
    "    \n",
    "    # Append simulations and train the likelihood estimator\n",
    "    density_estimator = inference.append_simulations(theta, x_ss).train()\n",
    "    \n",
    "    # Build the posterior (combines estimated likelihood with prior)\n",
    "    posterior = inference.build_posterior(mcmc_method=\"slice_np_vectorized\",\n",
    "                                          mcmc_parameters={\"num_chains\": 20, \"thin\": 5})\n",
    "    \n",
    "    # Update the proposal to focus on high-probability regions\n",
    "    proposal = posterior.set_default_x(x_o_ss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_z/ms2f3nmn0bb7wk4py9_pz7900000gp/T/ipykernel_18616/775500019.py:5: UserWarning: You passed `mcmc_method` to `.sample()`. As of sbi v0.18.0, this is deprecated and will be removed in a future release. Use `method` instead of `mcmc_method`.\n",
      "  samples = posterior.sample((num_posterior_samples,),\n",
      "/var/folders/_z/ms2f3nmn0bb7wk4py9_pz7900000gp/T/ipykernel_18616/775500019.py:5: UserWarning: You passed `mcmc_parameters` to `.sample()`. As of sbi v0.18.0, this is deprecated and will be removed in a future release. Instead, pass the variable to `.sample()` directly, e.g. `posterior.sample((1,), num_chains=5)`.\n",
      "  samples = posterior.sample((num_posterior_samples,),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07dc8008e48244949fab015933eeb0e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running vectorized MCMC with 20 chains:   0%|          | 0/71000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Number of posterior samples\n",
    "num_posterior_samples = 10_000\n",
    "\n",
    "# Sample from the posterior given the observed data\n",
    "samples = posterior.sample((num_posterior_samples,),\n",
    "                           x=x_o_ss,\n",
    "                           mcmc_method=\"slice_np_vectorized\",\n",
    "                           mcmc_parameters={\"num_chains\": 20, \"thin\": 5})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert samples to NumPy array for plotting\n",
    "# samples_np = samples.numpy().flatten()\n",
    "\n",
    "# # Plot the posterior samples\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# plt.hist(samples_np, bins=50, density=True, alpha=0.6, label='NLE Posterior Samples')\n",
    "\n",
    "# # Plot the prior distribution\n",
    "# theta_range = np.linspace(-1, 1, 200)\n",
    "# prior_density = np.ones_like(theta_range) * 0.5  # Uniform density\n",
    "# plt.plot(theta_range, prior_density, label='Prior', color='black', linestyle='--')\n",
    "\n",
    "# # Plot the pseudo-true parameter value\n",
    "# # Since the model is misspecified, we may consider a pseudo-true value\n",
    "# theta_pseudo_true = 0.0  # Adjust based on your context\n",
    "# plt.axvline(theta_pseudo_true, color='red', linestyle='--', label='Pseudo-true θ')\n",
    "\n",
    "# plt.xlabel(r'$\\theta$')\n",
    "# plt.ylabel('Density')\n",
    "# plt.title('Posterior Distribution of $\\theta$ using NLE')\n",
    "# plt.legend()\n",
    "# plt.tight_layout()\n",
    "# plt.savefig(\"figs/nle_posterior.pdf\")\n",
    "# plt.clf()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 1])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot the joint distribution of (s0, s1)\n",
    "\n",
    "# x_pp_ss = np.empty((1000, 2))\n",
    "\n",
    "# for i in range(1000):\n",
    "#     theta = samples[i, :]\n",
    "#     x_sim = assumed_dgp(torch.atleast_2d(theta),n_obs=n_obs)\n",
    "#     x_ss = calculate_summary_statistics(x_sim)\n",
    "#     x_pp_ss[i, :] = x_ss\n",
    "\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# plt.scatter(x_pp_ss[:, 1], x_pp_ss[:, 0], alpha=0.5, label='Posterior Predictive')\n",
    "# plt.scatter(x_o_ss[0, 1].item(), x_o_ss[0, 0].item(), color='red', marker='x', s=100, label='Observed Data')\n",
    "# plt.xlabel('$s_0$ (Autocovariance at lag 0)')\n",
    "# plt.ylabel('$s_1$ (Autocovariance at lag 1)')\n",
    "# plt.title('Posterior Predictive Summary Statistics')\n",
    "# plt.legend()\n",
    "# plt.tight_layout()\n",
    "# plt.savefig(\"figs/posterior_predictive_joint_nle.pdf\")\n",
    "# plt.clf()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Update font size for consistency\n",
    "plt.rcParams.update({'font.size': 24})\n",
    "\n",
    "# Assume that `samples` contains the posterior samples from NLE\n",
    "# and that `assumed_dgp` and `calculate_summary_statistics` are defined\n",
    "# Also, `x_o_ss` contains the observed summary statistics\n",
    "\n",
    "# Convert samples to NumPy array for plotting\n",
    "samples_np = samples.numpy().flatten()\n",
    "\n",
    "# Plot the histogram of posterior samples\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.hist(samples_np, bins=50, density=True, alpha=0.6, color='blue', label='Posterior Samples')\n",
    "\n",
    "# Plot the prior distribution for comparison\n",
    "theta_range = np.linspace(-1, 1, 200)\n",
    "\n",
    "\n",
    "\n",
    "# Plot the pseudo-true parameter value\n",
    "theta_pseudo_true = 0.0  # Adjust based on your context\n",
    "plt.axvline(theta_pseudo_true, color='black', linestyle='--', label='Pseudo-true θ')\n",
    "\n",
    "# Add labels and legend\n",
    "plt.xlabel(r'$\\theta$')\n",
    "plt.ylabel('Density')\n",
    "plt.xlim([-1, 1])\n",
    "plt.xticks([-1, 0, 1])\n",
    "# plt.title('Posterior Distribution of $\\theta$ using NLE')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"figs/fig3a_nle.pdf\")\n",
    "plt.clf()\n",
    "\n",
    "# Posterior predictive simulations\n",
    "num_posterior_samples = len(samples_np)\n",
    "num_pp_samples = 1_000  # Number of posterior predictive samples\n",
    "thinning_interval = num_posterior_samples // num_pp_samples\n",
    "\n",
    "# Initialize array to store summary statistics\n",
    "x_pp_ss = np.empty((num_pp_samples, 2))\n",
    "\n",
    "# Perform posterior predictive simulations\n",
    "for i in range(num_pp_samples):\n",
    "    idx = i * thinning_interval\n",
    "    theta = samples_np[idx]\n",
    "    x_sim = assumed_dgp(torch.tensor([[theta]]), n_obs=100)  # Adjust n_obs as needed\n",
    "    x_ss = calculate_summary_statistics(x_sim)\n",
    "    x_pp_ss[i, :] = x_ss.numpy()\n",
    "\n",
    "# Plot the joint distribution of summary statistics from posterior predictive simulations\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(x_pp_ss[:, 1], x_pp_ss[:, 0], c='blue', alpha=0.5)\n",
    "def b_theta(t):\n",
    "    return np.array([1 + t ** 2, t])\n",
    "\n",
    "# Generate values for b_theta\n",
    "t_vals = np.linspace(-1, 1, 101)\n",
    "b_theta_vals = np.array([b_theta(t) for t in t_vals])\n",
    "\n",
    "# prior_density = np.ones_like(theta_range) * 0.5  # Uniform density over [-1, 1]\n",
    "plt.plot(b_theta_vals[:, 1], b_theta_vals[:, 0], color='orange', label=r'$b(\\theta)$', linewidth=6)\n",
    "\n",
    "\n",
    "# Plot the observed summary statistics\n",
    "plt.scatter(x_o_ss[0, 1].item(), x_o_ss[0, 0].item(), color='black', marker='x', s=100, label='S(y)')\n",
    "\n",
    "# Include the function b_theta for reference, if applicable\n",
    "# plt.plot(b_theta_vals[:, 1], b_theta_vals[:, 0], color='orange', label=r'$b(\\theta)$', linewidth=4)\n",
    "\n",
    "# Set axis labels\n",
    "plt.xlabel(r'$\\zeta_1$')\n",
    "plt.ylabel(r'$\\zeta_2$')\n",
    "plt.xlim(-2.0, 2.0)\n",
    "plt.ylim(-0.5, 2.5)\n",
    "plt.xticks([-2, 0, 2])\n",
    "plt.yticks([-0.5, 1, 2.5])\n",
    "\n",
    "# plt.title('Posterior Predictive Summary Statistics')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"figs/fig3b_nle.pdf\")\n",
    "plt.clf()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
